# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jlmWlZMAjmfc2PezuuKDdSbtzShk5qsS

# Proyek Pertama : Predictive Analytics
- **Nama:** Muhammad Alivian Sidiq
- **Email:** alivian7373@gmail.com
- **ID Dicoding:** alivian_7

# Data Understanding

## Import Library/Pakages

Improt semua library yang digunakan
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import warnings
warnings.simplefilter(action='ignore')
import matplotlib.pyplot as plt
plt.style.use("ggplot")
# %matplotlib inline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import GradientBoostingClassifier
from imblearn.over_sampling import SMOTE
from sklearn import metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix,classification_report,roc_curve, auc

"""## Data Loading
Sumber : Pima Indians Diabetes Database https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database <br>
Read the Dataset
"""

df = pd.read_csv("/content/diabetes.csv")
df

"""## Deskripsi Variabel
Berdasarkan informasi dari sumber dataset berikut adalah penjelasan untuk masing-masing kolom :

| No | Column Name |	Meaning |
|----|-------------|----------|
|1|Pregnancies|	Number of pregnancies the individual has had.
|2|	Glucose|	Plasma glucose concentration measured in a 2-hour oral glucose tolerance test.
|3|	BloodPressure|	Diastolic blood pressure measurement (mm Hg).
|4|	SkinThickness|	Triceps skin fold thickness (mm).
|5|	Insulin|	2-hour serum insulin (mu U/ml).
|6|	BMI	|Body mass index, calculated as weight in kg/(height in meters)^2.
|7|	DiabetesPedigreeFunction|	A function which scores the likelihood of diabetes based on family history.
|8|	Age|	Age of the individual in years.
|9|	Outcome|	This is the target variable, indicating whether the individual has diabetes (1) or not (0)

# Exploratory Data Analysis (EDA)

Menampilkan informasi tentang dataframe
"""

df.info()

df.columns

"""Deskripsi Statistik"""

df.describe()

"""Grafik dari kolom 'Outcome' yang menampilkan distribusi jumlah dari No Diabeses dan Diabetes"""

sns.countplot(x='Outcome', data=df)
plt.title('Distribution of Outcome (Diabetes)')
plt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)')
plt.ylabel('Count')
plt.show()

"""Menampilkan korelasi antar kolom"""

df.corr()

"""Interpretasi analisis korelasi yang dilakukan menunjukkan bahwa kolom outcome memiliki korelasi paling tinggi dengan kolom glukosa dengan skor korelasi sebesar 0,466. Artinya terdapat hubungan yang cukup kuat antara kadar glukosa dengan outcome diabetes, yang menunjukkan bahwa semakin tinggi kadar glukosa maka semakin besar kemungkinan seseorang menderita diabetes.

## Data Visualization
Heatmap korelasi matriks antar kolom
"""

plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""Boxplot untuk memperlihatkan distribusi data, nilai outlier, dan nilai kuartil suatu variabel."""

fig, axes = plt.subplots(3, 3, figsize=(15, 10))

for i, feature in enumerate(df.columns):
    row = i // 3
    col = i % 3
    sns.boxplot( x=feature, data=df, ax=axes[row, col])
    axes[row, col].set_title(f'Outlier Detection for {feature}')

plt.tight_layout()
plt.show()

"""Violin Plot"""

fig, axes = plt.subplots(3, 3, figsize=(15, 15))

for i, feature in enumerate(df.columns[:-1]):  # Exclude 'Outcome'
    row = i // 3
    col = i % 3
    sns.violinplot(x='Outcome', y=feature, data=df, ax=axes[row, col])
    axes[row, col].set_title(f'Violin Plot for {feature}')

plt.tight_layout()
plt.show()

"""#Data Preparation

## Cleaning Data
Menganti nilai 0 dangan NaN
"""

df[[ 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']] = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']].replace(0, np.NaN)
df.isnull().sum()

"""Menghitung nilai NaN"""

df.isnull().sum()

df.head()

"""## Imputation

Imputasi pada setiap kolom numerik kecuali kolom “Outcome” berdasarkan nilai median kolom tersebut tergantung pada nilai “Outcome” (0 , 1)
"""

def median_target(var):
    temp = df[df[var].notnull()]
    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()
    return temp

columns = df.columns
columns = columns.drop("Outcome")
for i in columns:
    median_target(i)
    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]
    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]

df.head()

df.isnull().sum()

"""## Handling Outlier with LOF
Deteksi outlier setiap kolom
"""

for feature in df:
    Q1 = df[feature].quantile(0.25)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3-Q1
    lower = Q1-1.5*IQR
    upper = Q3+1.5*IQR
    if df[(df[feature]>upper)].any(axis=None):
        print(feature, "yes")
    else:
        print(feature, "no")

"""Mendeteksi outlier menggunakan metode LOF (untuk mendeteksi outlier berdasarkan "local density deviation" dari suatu observasi ke observasi tetangganya.)"""

lof = LocalOutlierFactor(n_neighbors=10)
lof.fit_predict(df)

"""Dapatkan 20 nilai terkecil dari skor faktor outlier negatif yang dihasilkan oleh model LOF (Local Outlier Factor). Skor ini menunjukkan seberapa jauh setiap titik data dari tetangganya dalam konteks kepadatan lokal"""

df_scores = lof.negative_outlier_factor_
np.sort(df_scores)[0:20]

thresold = np.sort(df_scores)[7]
thresold

outlier = df_scores>thresold

"""Menghapus outlier berdasarkan nilai yang diperoleh dari model LOF (Local Outlier Factor)"""

df = df[outlier]

df.head()

"""## Feature Engginering

Buat kolom BMI baru menjadi kategorikal
"""

NewBMI = pd.Series(["Underweight","Normal", "Overweight","Obesity 1", "Obesity 2", "Obesity 3"], dtype = "category")

df['NewBMI'] = NewBMI
df.loc[df["BMI"]<18.5, "NewBMI"] = NewBMI[0]
df.loc[(df["BMI"]>18.5) & df["BMI"]<=24.9, "NewBMI"] = NewBMI[1]
df.loc[(df["BMI"]>24.9) & df["BMI"]<=29.9, "NewBMI"] = NewBMI[2]
df.loc[(df["BMI"]>29.9) & df["BMI"]<=34.9, "NewBMI"] = NewBMI[3]
df.loc[(df["BMI"]>34.9) & df["BMI"]<=39.9, "NewBMI"] = NewBMI[4]
df.loc[df["BMI"]>39.9, "NewBMI"] = NewBMI[5]

"""Buat kolom Insulin baru menjadi kategorikal"""

def set_insuline(row):
    if row["Insulin"]>=16 and row["Insulin"]<=166:
        return "Normal"
    else:
        return "Abnormal"

df = df.assign(NewInsulinScore=df.apply(set_insuline, axis=1))

"""Buat kolom Glukosa baru menjadi kategorikal"""

# Some intervals were determined according to the glucose variable and these were assigned categorical variables.
NewGlucose = pd.Series(["Low", "Normal", "Overweight", "Secret", "High"], dtype = "category")
df["NewGlucose"] = NewGlucose
df.loc[df["Glucose"] <= 70, "NewGlucose"] = NewGlucose[0]
df.loc[(df["Glucose"] > 70) & (df["Glucose"] <= 99), "NewGlucose"] = NewGlucose[1]
df.loc[(df["Glucose"] > 99) & (df["Glucose"] <= 126), "NewGlucose"] = NewGlucose[2]
df.loc[df["Glucose"] > 126 ,"NewGlucose"] = NewGlucose[3]

"""## Encoding

Encoding kolom "NewBMI", "NewInsulinScore", "NewGlucose" dengan One-Hot Encoding
"""

df = pd.get_dummies(df, columns = ["NewBMI", "NewInsulinScore", "NewGlucose"], drop_first=True)

df.head()

"""Pisahkan kolom numerikal dan kategorikal"""

categorical_df = df[['NewBMI_Obesity 1',
       'NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight',
       'NewBMI_Underweight', 'NewInsulinScore_Normal', 'NewGlucose_Low',
       'NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]

y=df['Outcome']
X=df.drop(['Outcome','NewBMI_Obesity 1',
       'NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight',
       'NewBMI_Underweight', 'NewInsulinScore_Normal', 'NewGlucose_Low',
       'NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret'], axis=1)

cols = X.columns
index = X.index

"""## Scaling Data

Scaling data menggunakan MinMaxScaler yang merubah data menjadi rentang 0-1
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler().fit(X)
X = scaler.transform(X)
X = pd.DataFrame(X, columns=cols, index=index)
X.head()

X = pd.concat([X, categorical_df], axis=1)

"""## Handling Imbalance Class

Penanganan Kelas Imblanace dalam Dataset menggunakan SMOTE
"""

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

count_class_0 = (y_resampled == 0).sum()
print("Count Class 0 after SMOTE:", count_class_0)
count_class_1 = (y_resampled == 1).sum()
print("Count Class 1 after SMOTE:", count_class_1)

"""## Split Data Train & Test

Data train = 80% Data test = 20%
"""

X_train, X_test, y_train , y_test = train_test_split(X_resampled,y_resampled, test_size=0.2, random_state=42)

X_train.columns = X_train.columns.str.replace(' ', '_')
X_test.columns = X_test.columns.str.replace(' ', '_')

"""# Modelling

Menggunakan Algoritma Machine Learning Random Forest, SVM , Logistic Regression, Gradient Boosting

## Random Forest
"""

# Hyperparameter Tunnning
rand_clf = RandomForestClassifier(random_state=42)
param_grid = {
    'n_estimators': [100, 130, 150],
    'criterion': ['gini', 'entropy'],
    'max_depth': [10, 15, 20, None],
    'max_features': [0.5, 0.75, 'sqrt', 'log2'],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3]
}
grid_search = GridSearchCV(rand_clf, param_grid, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model_rf = grid_search.best_estimator_
y_pred = best_model_rf.predict(X_test)

rand_acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

rand_acc_percent = rand_acc * 100

print(f"Accuracy Score: {rand_acc_percent:.2f}%")
print(classification_report(y_test, y_pred))

"""## Logistic Regression"""

## Hyperparameter Tunnning
log_reg = LogisticRegression(random_state=42, max_iter=3000)

param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
}

grid_search = GridSearchCV(log_reg, param_grid, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model_lr = grid_search.best_estimator_
y_pred = best_model_lr.predict(X_test)
log_reg_acc = accuracy_score(y_test, best_model_lr.predict(X_test))

print("Accuracy Score:", log_reg_acc)
print(classification_report(y_test, y_pred))

"""## SVM"""

# Hyperparameter Tunnning
svc = SVC(probability=True, random_state=42)
parameter = {
    "gamma":[0.0001, 0.001, 0.01, 0.1],
    'C': [0.01, 0.05,0.5, 0.01, 1, 10, 15, 20]
}
grid_search = GridSearchCV(svc, parameter, n_jobs=-1)
grid_search.fit(X_train, y_train)

svc_best = grid_search.best_estimator_
svc_best.fit(X_train, y_train)
y_pred = svc_best.predict(X_test)

svc_acc = accuracy_score(y_test, y_pred)

print("Accuracy Score:", svc_acc)
print(classification_report(y_test, y_pred))

"""
## Gradient Boosting"""

# Hyperparameter Tunnning

gb_clf = GradientBoostingClassifier(random_state=42)

param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3]
}

grid_search = GridSearchCV(gb_clf, param_grid, n_jobs=-1, cv=5)
grid_search.fit(X_train, y_train)

best_model_gb = grid_search.best_estimator_
best_model_gb.fit(X_train, y_train)
y_pred = best_model_gb.predict(X_test)
gb_acc = accuracy_score(y_test, y_pred)

print("Accuracy Score:", gb_acc)
print(classification_report(y_test, y_pred))

"""#Evaluation

Perbandingan Performa model berdasarkan Akurasi
"""

models = pd.DataFrame({
    'Model': ['Logistic Regression', 'SVM', 'Gradient Boosting', 'Random Forest'],
    'Score': [100*round(log_reg_acc,4), 100*round(svc_acc,4), 100*round(gb_acc,4), 100*round(rand_acc,4)]
})
# Mengurutkan DataFrame berdasarkan skor
models_sorted = models.sort_values(by='Score', ascending=False)

# Visualisasi data dengan Seaborn
plt.figure(figsize=(10, 6))
sns.barplot(x='Score', y='Model', data=models_sorted, palette='viridis')
plt.title('Model Accuracy Comparison')
plt.xlabel('Accuracy (%)')
plt.ylabel('Model')
plt.xlim(0, 100)

# Menambahkan teks skor di setiap batang
for index, value in enumerate(models_sorted['Score']):
    plt.text(value, index, f'{value:.2f}%', va='center')

"""Perbandingan Performa model berdasarkan ROC"""

def plot_roc_curves(models, model_names, X_test, y_test):
    plt.figure(figsize=(10, 8))
    for model, model_name in zip(models, model_names):
        y_probs = model.predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds = roc_curve(y_test, y_probs)
        roc_auc = auc(fpr, tpr)

        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='gray', label='Random Guessing')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curves')
    plt.legend(loc='lower right')
    plt.show()

models = [best_model_rf, best_model_lr, svc_best, best_model_gb]
model_names = ['Random Forest', 'Logistic Regression', 'SVM', 'Gradient Boosting']

plot_roc_curves(models, model_names, X_test, y_test)

"""Perbandingan Metrik Evaluasi Model secara terurut"""

models = {
    'Random Forest': best_model_rf,
    'SVM': svc_best,
    'Gradient Boosting' :best_model_gb,
    'Logistic Regression': best_model_lr,
}

def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')
    return accuracy, precision, recall, f1

results = []

for model_name, model in models.items():
    accuracy, precision, recall, f1 = evaluate_model(model, X_train, X_test, y_train, y_test)
    results.append({
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    })

results_df = pd.DataFrame(results)

metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
sorted_dfs = {metric: results_df.sort_values(by=metric, ascending=False) for metric in metrics}

melted_dfs = []
for metric, sorted_df in sorted_dfs.items():
    sorted_df['Rank'] = range(1, len(sorted_df) + 1)
    melted_df = pd.melt(sorted_df, id_vars=['Model', 'Rank'], value_vars=[metric],
                        var_name='Metric', value_name='Score')
    melted_dfs.append(melted_df)

results_melted = pd.concat(melted_dfs)

plt.figure(figsize=(12, 8))
ax = sns.barplot(x='Metric', y='Score', hue='Model', data=results_melted, order=metrics)
plt.title('Comparison of Evaluation Metrics by Model (Sorted)')
plt.xlabel('Metric')
plt.ylabel('Score')
plt.legend(title='Model', loc='upper right', bbox_to_anchor=(1.2, 1))

for p in ax.patches:
    ax.annotate(f"{p.get_height():.3f}", (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.show()